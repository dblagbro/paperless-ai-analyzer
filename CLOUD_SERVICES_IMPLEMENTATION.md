# Cloud Services & URL Polling Implementation

**Date:** 2026-02-16
**Status:** ‚úÖ Code Complete - Ready for Container Rebuild & Testing
**Phase:** 2A (Infrastructure) + 2B (Google Drive) + 2C (URL Polling) + 2D (Dropbox, OneDrive, S3)

---

## üéØ What Was Implemented

### Phase 2A: Cloud Service Infrastructure ‚úÖ
- Base adapter class (abstract interface)
- Unified API for all cloud services
- Consistent file listing, downloading, metadata extraction

### Phase 2B: Google Drive Integration ‚úÖ
- OAuth2 user authentication support
- Service account authentication support
- Folder browsing and file listing
- Google Workspace file export (Docs ‚Üí PDF, Sheets ‚Üí XLSX)
- Search functionality

### Phase 2C: Periodic URL Polling ‚úÖ
- SQLite database for tracked URLs
- Background polling thread
- Content change detection (SHA256 hashing)
- Import history tracking
- Auto-import to specified projects

### Phase 2D: Other Cloud Services ‚úÖ
- **Dropbox:** Full API integration with OAuth tokens
- **OneDrive/SharePoint:** Microsoft Graph API integration
- **Amazon S3:** Boto3 integration with bucket browsing

---

## üì¶ New Components Created

### 1. Cloud Adapter Base Class
**File:** `analyzer/cloud_adapters/base.py` (100 lines)

```python
class CloudServiceAdapter(ABC):
    @abstractmethod
    async def authenticate() -> bool
    @abstractmethod
    async def list_files(folder_path, page_token) -> Dict
    @abstractmethod
    async def download_file(file_id, output_path) -> str
    @abstractmethod
    async def get_file_metadata(file_id) -> Dict
    async def search_files(query, folder_path) -> List[Dict]
```

**Standard File Dict Format:**
```python
{
    'id': 'unique_file_id',
    'name': 'filename.pdf',
    'size': 12345,  # bytes
    'modified': '2024-01-01T00:00:00Z',  # ISO format
    'mime_type': 'application/pdf',
    'is_folder': False
}
```

---

### 2. Google Drive Adapter
**File:** `analyzer/cloud_adapters/google_drive.py` (280 lines)

**Features:**
- ‚úÖ OAuth2 access token authentication
- ‚úÖ Service account JSON authentication
- ‚úÖ List files/folders with pagination
- ‚úÖ Download files
- ‚úÖ Export Google Workspace files:
  - Google Docs ‚Üí PDF
  - Google Sheets ‚Üí XLSX
  - Google Slides ‚Üí PPTX
- ‚úÖ Search by filename
- ‚úÖ Get file metadata

**Authentication Options:**

**Option A: OAuth2 User Token**
```python
credentials = {
    'access_token': 'ya29.a0AfH6SMBx...',
    'refresh_token': '1//0gX...',  # Optional
    'client_id': '123.apps.googleusercontent.com',
    'client_secret': 'abc123'
}
```

**Option B: Service Account**
```python
credentials = {
    'service_account_json': '/path/to/service-account.json'
}
# OR
credentials = {
    'service_account_info': {
        'type': 'service_account',
        'project_id': 'my-project',
        'private_key_id': '...',
        'private_key': '-----BEGIN PRIVATE KEY-----...',
        'client_email': 'bot@project.iam.gserviceaccount.com',
        ...
    }
}
```

**Usage:**
```python
from analyzer.cloud_adapters import GoogleDriveAdapter

adapter = GoogleDriveAdapter(credentials)
await adapter.authenticate()

# List files in folder
result = await adapter.list_files(folder_path='folder_id')
for file in result['files']:
    print(f"{file['name']} ({file['size']} bytes)")

# Download file
await adapter.download_file(file_id='abc123', output_path='/tmp/file.pdf')
```

---

### 3. Dropbox Adapter
**File:** `analyzer/cloud_adapters/dropbox_adapter.py` (210 lines)

**Features:**
- ‚úÖ Access token authentication
- ‚úÖ List files/folders with pagination
- ‚úÖ Download files
- ‚úÖ Search by filename
- ‚úÖ Get file metadata

**Authentication:**
```python
credentials = {
    'access_token': 'sl.ABC123...'
}
```

**Note:** Dropbox uses paths as IDs (e.g., `/documents/file.pdf`)

---

### 4. OneDrive/SharePoint Adapter
**File:** `analyzer/cloud_adapters/onedrive.py` (230 lines)

**Features:**
- ‚úÖ Microsoft Graph API integration
- ‚úÖ OAuth2 token authentication
- ‚úÖ List files/folders with pagination
- ‚úÖ Download files
- ‚úÖ Search by filename
- ‚úÖ Get file metadata

**Authentication:**
```python
credentials = {
    'access_token': 'eyJ0eXAiOiJKV1QiLCJhbGc...'
}
```

**Note:** Works with both OneDrive personal and OneDrive for Business/SharePoint

---

### 5. Amazon S3 Adapter
**File:** `analyzer/cloud_adapters/s3.py` (240 lines)

**Features:**
- ‚úÖ AWS credentials authentication
- ‚úÖ List objects/folders with pagination
- ‚úÖ Download objects
- ‚úÖ Prefix-based search
- ‚úÖ Get object metadata
- ‚úÖ S3-compatible storage support (custom endpoints)

**Authentication:**
```python
credentials = {
    'aws_access_key_id': 'AKIAIOSFODNN7EXAMPLE',
    'aws_secret_access_key': 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY',
    'bucket_name': 'my-documents',
    'region_name': 'us-east-1',  # Optional
    'endpoint_url': 'https://s3.custom.com'  # Optional (for S3-compatible)
}
```

---

### 6. URL Poller
**File:** `analyzer/url_poller.py` (380 lines)

**Features:**
- ‚úÖ SQLite database for tracked URLs
- ‚úÖ Periodic background polling
- ‚úÖ Content change detection (SHA256 hash)
- ‚úÖ Import history tracking
- ‚úÖ Per-URL configuration:
  - Poll interval (hours)
  - Authentication settings
  - Target project
  - Enable/disable toggle
- ‚úÖ Auto-import new/changed documents

**Database Schema:**

**tracked_urls table:**
```sql
CREATE TABLE tracked_urls (
    id INTEGER PRIMARY KEY,
    url TEXT NOT NULL UNIQUE,
    auth_type TEXT DEFAULT 'none',
    username TEXT,
    password TEXT,
    token TEXT,
    custom_headers TEXT,
    project_slug TEXT NOT NULL,
    poll_interval_hours INTEGER DEFAULT 24,
    last_checked TEXT,
    last_content_hash TEXT,
    last_modified TEXT,
    enabled INTEGER DEFAULT 1,
    created_at TEXT,
    notes TEXT
)
```

**import_history table:**
```sql
CREATE TABLE import_history (
    id INTEGER PRIMARY KEY,
    tracked_url_id INTEGER,
    imported_at TEXT,
    filename TEXT,
    size_bytes INTEGER,
    content_hash TEXT,
    paperless_doc_id INTEGER,
    status TEXT,  -- 'success' or 'error'
    error TEXT
)
```

**Usage:**
```python
from analyzer.url_poller import URLPoller

poller = URLPoller(downloader=downloader, smart_uploader=smart_uploader)

# Add URL to track
poller.add_tracked_url(
    url='https://court.example.gov/docket/123/filings.pdf',
    project_slug='case-2024-123',
    auth_type='basic',
    username='user@firm.com',
    password='secret',
    poll_interval_hours=6,  # Check every 6 hours
    notes='Court docket filings'
)

# Start background polling
poller.start_polling(check_interval_seconds=300)  # Check every 5 min for due URLs

# List tracked URLs
urls = poller.list_tracked_urls()

# Get import history
history = poller.get_import_history(url_id=1)
```

**How It Works:**
1. Background thread runs every 5 minutes (configurable)
2. Checks which tracked URLs are "due" (last_checked + poll_interval_hours <= now)
3. For each due URL:
   - Downloads file with configured authentication
   - Calculates SHA256 hash of content
   - Compares with last_content_hash
   - If changed:
     - Analyzes with AI
     - Uploads to Paperless in specified project
     - Records import in history
   - Updates last_checked and last_content_hash

---

## üìä Files Created/Modified

### Created (New Files):
- `analyzer/cloud_adapters/__init__.py` (20 lines)
- `analyzer/cloud_adapters/base.py` (100 lines)
- `analyzer/cloud_adapters/google_drive.py` (280 lines)
- `analyzer/cloud_adapters/dropbox_adapter.py` (210 lines)
- `analyzer/cloud_adapters/onedrive.py` (230 lines)
- `analyzer/cloud_adapters/s3.py` (240 lines)
- `analyzer/url_poller.py` (380 lines)
- `analyzer/remote_downloader.py` (330 lines - from Phase 1)
- `CLOUD_SERVICES_IMPLEMENTATION.md` (this file)

### Modified:
- `requirements.txt` (+16 lines)
  - Added: google-api-python-client, google-auth, dropbox, boto3, aiohttp

**Total New Code:** ~1,790 lines

---

## üîê Authentication Summary

### Google Drive
**Method 1: OAuth2 (User Authentication)**
- User logs in with Google account
- Get access_token + refresh_token
- App accesses user's Drive files
- **Pro:** User-specific permissions
- **Con:** Requires OAuth flow, tokens expire

**Method 2: Service Account (Bot Authentication)**
- Create service account in Google Cloud Console
- Download JSON key file
- Share Drive folders with service account email
- **Pro:** No user interaction, tokens don't expire
- **Con:** Need to share folders explicitly

### Dropbox
**Method: OAuth2 Access Token**
- Create Dropbox app
- Get access token via OAuth flow
- Token provides access to user's Dropbox
- Long-lived tokens available

### OneDrive/SharePoint
**Method: Microsoft Graph OAuth2**
- Register app in Azure AD
- Get access token via OAuth flow
- Token accesses user's OneDrive/SharePoint
- Supports both personal and business accounts

### Amazon S3
**Method: AWS Credentials**
- AWS Access Key ID + Secret Access Key
- Can use IAM user or role credentials
- Works with S3-compatible services (Minio, DigitalOcean Spaces, etc.)

---

## üöÄ Next Steps (Not Yet Implemented)

### API Endpoints (TODO)
Need to add API endpoints for:
- `POST /api/cloud/connect` - Test connection to cloud service
- `POST /api/cloud/{service}/list` - List files in folder
- `POST /api/cloud/{service}/download` - Download and analyze file
- `POST /api/url-poller/add` - Add tracked URL
- `GET /api/url-poller/list` - List tracked URLs
- `DELETE /api/url-poller/{id}` - Remove tracked URL
- `GET /api/url-poller/{id}/history` - Get import history
- `PUT /api/url-poller/{id}` - Update tracked URL settings

### UI Components (TODO)
Need to add UI for:
- **Cloud Services Tab:**
  - Service selector (Google Drive, Dropbox, OneDrive, S3)
  - Credentials input form
  - "Connect" button
  - File browser (folder navigation, file list)
  - Multi-select checkboxes
  - "Import Selected" button

- **URL Polling Tab:**
  - "Add Tracked URL" button ‚Üí modal form
  - List of tracked URLs with status
  - Enable/disable toggles
  - "Check Now" buttons (force immediate check)
  - Import history view

### Integration with Main App (TODO)
Need to initialize in `main.py`:
- Create URLPoller instance
- Pass downloader + smart_uploader
- Start polling thread
- Pass poller to web UI

---

## üß™ Testing Plan

### Unit Tests (Manual):
**Google Drive:**
- [ ] Service account authentication
- [ ] OAuth2 authentication
- [ ] List root folder
- [ ] List specific folder
- [ ] Download regular file
- [ ] Download Google Doc (export as PDF)
- [ ] Search files

**Dropbox:**
- [ ] Token authentication
- [ ] List root folder
- [ ] Download file
- [ ] Search files

**OneDrive:**
- [ ] Token authentication
- [ ] List root folder
- [ ] Download file
- [ ] Search files

**S3:**
- [ ] Credentials authentication
- [ ] List bucket root
- [ ] List folder (prefix)
- [ ] Download object
- [ ] Search by prefix

**URL Poller:**
- [ ] Add tracked URL
- [ ] List tracked URLs
- [ ] Check URL (unchanged content)
- [ ] Check URL (changed content ‚Üí import)
- [ ] Background polling loop
- [ ] View import history

---

## üìñ User Documentation

### How to Use Google Drive Integration:

**Option A: Service Account (Easiest for Shared Folders)**

1. **Create Service Account:**
   - Go to [Google Cloud Console](https://console.cloud.google.com)
   - Create new project or select existing
   - Enable Google Drive API
   - Create Service Account
   - Download JSON key file

2. **Share Folders:**
   - Open Google Drive folder you want to access
   - Share with service account email (looks like `bot@project.iam.gserviceaccount.com`)
   - Give "Viewer" permission

3. **Use in App:**
   - Upload service account JSON to server
   - OR copy JSON content
   - Connect in UI with service account credentials

**Option B: OAuth2 (User-Specific Access)**

1. **Get OAuth Token:**
   - Use Google OAuth2 flow (requires web app setup)
   - Get access_token from OAuth callback
   - Optionally get refresh_token

2. **Use in App:**
   - Enter access_token in UI
   - (Future: Add "Connect with Google" button for full OAuth flow)

### How to Use Dropbox:

1. **Get Access Token:**
   - Go to [Dropbox App Console](https://www.dropbox.com/developers/apps)
   - Create app or use existing
   - Generate access token

2. **Use in App:**
   - Enter access token in UI
   - Browse files, download to Paperless

### How to Use OneDrive:

1. **Get Access Token:**
   - Register app in [Azure Portal](https://portal.azure.com)
   - Add Microsoft Graph permissions: `Files.Read.All`
   - Use OAuth2 flow to get token

2. **Use in App:**
   - Enter access token in UI
   - Browse OneDrive/SharePoint, import files

### How to Use S3:

1. **Get AWS Credentials:**
   - Create IAM user with S3 read permissions
   - Generate Access Key ID + Secret Access Key
   - Note bucket name and region

2. **Use in App:**
   - Enter credentials in UI
   - Browse bucket, download objects

### How to Set Up URL Polling:

1. **Navigate to URL Polling Tab**

2. **Click "Add Tracked URL"**

3. **Fill Out Form:**
   - URL: `https://example.com/document.pdf`
   - Project: Select target project
   - Auth: Configure if needed
   - Poll Interval: e.g., 24 hours
   - Notes: Optional description

4. **Click "Add"**

5. **Monitor:**
   - URL will be checked automatically
   - View import history to see when documents were imported
   - Enable/disable as needed

---

## üîß Dependencies Added

```
# Google Drive
google-api-python-client>=2.100.0
google-auth>=2.23.0
google-auth-httplib2>=0.1.1
google-auth-oauthlib>=1.1.0

# Dropbox
dropbox>=11.36.0

# Amazon S3
boto3>=1.34.0

# Async HTTP
aiohttp>=3.9.0
```

**Installation Size:** ~50-100MB additional dependencies

---

## ‚ö†Ô∏è Important Notes

### Security:
- All credentials stored in memory/session only (not persisted to disk by default)
- URL poller stores credentials encrypted in SQLite (TODO: add encryption)
- HTTPS recommended for all API calls
- OAuth tokens should be refreshed periodically

### Performance:
- Cloud adapters use async/await for non-blocking I/O
- URL poller runs in background thread
- Large file downloads use streaming (memory-efficient)

### Limitations:
- Google Drive: Max 100 files per list request (pagination supported)
- Dropbox: Uses file paths as IDs (not numeric IDs)
- OneDrive: Requires Microsoft Graph permissions
- S3: Prefix search only (no full-text search)
- URL Poller: Content detection via hash (doesn't detect metadata changes only)

---

## üìù Status Summary

| Component | Status | Ready for UI |
|-----------|--------|--------------|
| Base Adapter | ‚úÖ Complete | N/A |
| Google Drive | ‚úÖ Complete | ‚è∏Ô∏è Need endpoints + UI |
| Dropbox | ‚úÖ Complete | ‚è∏Ô∏è Need endpoints + UI |
| OneDrive | ‚úÖ Complete | ‚è∏Ô∏è Need endpoints + UI |
| S3 | ‚úÖ Complete | ‚è∏Ô∏è Need endpoints + UI |
| URL Poller | ‚úÖ Complete | ‚è∏Ô∏è Need endpoints + UI |

**Overall:** Backend complete, API + UI pending

---

## üéØ Immediate Next Steps

1. **Add API Endpoints** (2-3 hours)
   - Cloud service endpoints
   - URL poller endpoints

2. **Add UI Components** (4-5 hours)
   - Cloud services tab
   - URL polling tab
   - File browser UI
   - Forms and modals

3. **Integration Testing** (1-2 hours)
   - Test each cloud service
   - Test URL polling
   - Test end-to-end workflows

4. **Container Rebuild** (5 minutes)
   - Rebuild with new dependencies
   - Test startup
   - Verify functionality

**Total Estimated Time to Full Feature:** 7-10 hours

---

**Implementation Status:** ‚úÖ Backend Complete (B, C, D)
**Next Phase:** API + UI Integration
**Ready for:** Container rebuild (dependencies added)
